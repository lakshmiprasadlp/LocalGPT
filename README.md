# ğŸ¤– LOCAL GPT â€“ Offline GenAI

Welcome to **LOCAL GPT**, a suite of **offline Generative AI hands-on projects** built using LangChain, Ollama, FAISS, and LLaMA 3.2.  
Each project demonstrates a key concept in building secure, local, and production-ready LLM applications â€” **without internet dependency**.

This main repository acts as a portfolio index of **4 standalone projects**, each implemented and hosted in a separate GitHub repository.

---

## ğŸ“ Project List

### âœ… [Project 1 â€“ Hands-On Setup: Local LLM Playground](https://github.com/lakshmiprasadlp/hands-on-ollama-.git)

> A foundation setup for running and interacting with LLaMA 3.2 locally using **Ollama** and **LangChain**.

**Highlights:**
- Run LLMs like `llama3`, `mistral`, and `deepseek` locally
- Set up LangChain pipeline and LangSmith (optional)
- Zero cloud cost / Fully offline

---

### âœ… [Project 2 â€“ Offline Chatbot with LangChain, Ollama, and LLaMA 3.2](https://github.com/lakshmiprasadlp/Offline-Chatbot-with-Langchain-Ollama-LLAMA-3.2-.git)

> Build a privacy-focused chatbot with no internet required using local LLMs.

**Advantages:**
- ğŸ’¾ Fully offline and private
- ğŸ” No data leakage
- ğŸ” Multi-turn chat powered by local inference
- ğŸ’» Works on your own hardware

**Limitations:**
- ğŸ§  High RAM/CPU/GPU required
- ğŸ•“ Initial model load time can be long on CPU

---

### âœ… [Project 3 â€“ Chatbot with Memory using `ConversationSummaryBufferMemory`](https://github.com/lakshmiprasadlp/Conversational-Chat-Bot-Application-with-Langchain-Ollama.git)

> Improve chat consistency and context retention using memory in LangChain.

**Features:**
- Summary-based conversation memory
- Compresses history to reduce token cost
- Makes multi-turn chat intelligent and contextual
- Works with offline/local models (Ollama)

---

### âœ… [Project 4 â€“ Retrieval-Augmented Generation (RAG) with FAISS + PDF](https://github.com/lakshmiprasadlp/localRag.git)

> Ask deep questions over PDF documents using a full RAG pipeline.

**Pipeline:**
- ğŸ“„ Convert PDFs â†’ Markdown â†’ Chunks
- ğŸ” Embed using Ollama or OpenAI
- ğŸ“¦ Store vectors in FAISS
- ğŸ’¬ Retrieve top K chunks â†’ Answer using LLM (local or cloud)

**Use Case:**  
> Ask questions like â€œWhat is Googleâ€™s Q3 net income?â€ over earnings reports and get factual, context-aware answers.

---

## ğŸ“¦ Tech Stack Used Across Projects

| Category      | Tool/Library                   |
|---------------|--------------------------------|
| LLM           | Ollama (`llama3`, `deepseek`)  |
| Framework     | LangChain                      |
| Memory        | `ConversationSummaryBufferMemory` |
| Vector Store  | FAISS                          |
| Embeddings    | Ollama / OpenAI                |
| File Parser   | Docling                        |
| Prompting     | LangChain `ChatPromptTemplate` |
| Optional UI   | Streamlit / Gradio             |

---

## ğŸ¯ Objective

These projects aim to:
- Enable **offline GenAI** use-cases with full control and zero data risk.
- Demonstrate **LangChain concepts** hands-on: memory, RAG, embeddings, chains.
- Create a **portfolio-level suite** of GenAI use cases.

---

## ğŸ§  Next Steps / Future Work

- âœ… Add LangGraph for agentic workflows
- âœ… Streamlit UI for chatbot and RAG
- ğŸ”„ Integrate with long-term vector DB like Chroma
- ğŸŒ Serve locally via FastAPI/Gradio interface

---

## ğŸ™‹â€â™‚ï¸ Author

**Lakshmiprasad**  
Machine Learning & Generative AI Engineer  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/charanlp/) | ğŸŒ [GitHub](https://github.com/lakshmiprasadlp)

---

## ğŸ“„ License

This suite of hands-on GenAI projects is licensed under the **MIT License**.

---

> _â€œYour LLMs. Your Machine. Your Data.â€_ ğŸ’»ğŸ”’
